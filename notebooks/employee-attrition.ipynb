{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science pipeline in action to solve employee attrition problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code pattern is a high level overview of what to expect in a data science pipeline and tools that can be used along the way. It starts from framing business question to deploying the model. The pipeline is demonstrated through the employee attrition problem. \n",
    "\n",
    "Employees are the backbone of the organization. Organization's performance is heavily based on the quality of the employees. Challenges that an organization has to face due employee attrition are:\n",
    "\n",
    "1. Expensive in terms of both money and time to train new employees.\n",
    "2. Loss of experienced employees\n",
    "3. Impact in productivity\n",
    "4. Impact profit\n",
    "\n",
    "Before getting our hands dirty with the data, first step is to frame the business question. Having clarity on below questions is very crucial because the solution that is being developed will make sense only if we have well stated problem.\n",
    "\n",
    "“Good data science is more about the questions you pose of the data rather than data munging and analysis” — Riley Newman\n",
    "\n",
    "### Business questions to brainstorm:\n",
    "\n",
    "1. What factors are contributing more to employee attrition?\n",
    "2. What type of measures should the company take in order to retain their employees?\n",
    "3. What business value does the model bring?\n",
    "4. Will the model save lots of money?\n",
    "5. Which business unit faces the attrition problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Science Process Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/IBM/employee-attrition-aif360/raw/master/docs/source/images/pipeline.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above image explains the steps involved in solving a data science problem. It starts from data extraction to result interpretation. Once the model produces acceptable performance, it can be deployed in real-time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's solve employee attrition problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pygal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cufflinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aif360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all required libraries\n",
    "#Data Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "#Visulaization libraries\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.palettes import Viridis5\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pygal\n",
    "import cufflinks as cf\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "#model developemnt libraries\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "#Bias Mitigation libraries\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from IPython.display import Markdown, display\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "#\n",
    "from IPython.display import SVG, display\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#deployment library\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source: Kaggle\n",
    "- Data: IBM HR Analytics dataset (synthetically generated)\n",
    "  https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset/home\n",
    "- License: \n",
    "    *  Database: https://opendatacommons.org/licenses/odbl/1.0/\n",
    "    *  Contents: https://opendatacommons.org/licenses/dbcl/1.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule of thumb\n",
    "* Know all the available dataset for the problem (database/ internet/ third party etc) . Dataset must be reliable and   authentic.\n",
    "* Extract data in a format that can be used\n",
    "* Required Skills for data extraction in general(mostly used and not specific to the pattern):\n",
    "   - Distributed Storage: Hadoop, Apache Spark.\n",
    "   - Database Management: MySQL, PostgresSQL, MongoDB.\n",
    "   - Know to querying Relational Databases and retrieve unstructured Data like text, videos, audio files, documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/IBM/employee-attrition-aif360/raw/master/data/emp_attrition.csv --output-document=emp_attrition.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading csv file in pandas dataframe format. \n",
    "df_data = pd.read_csv('emp_attrition.csv')\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of columns in the dataset\n",
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns (intution)\n",
    "columns = ['DailyRate', 'EducationField', 'EmployeeCount', 'EmployeeNumber', 'HourlyRate', 'MonthlyRate',\n",
    "        'Over18', 'RelationshipSatisfaction', 'StandardHours']\n",
    "df_data.drop(columns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get description of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values.\n",
    "\n",
    "Reference link: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will give description only for numeric fields\n",
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get description of all columns\n",
    "df_data.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase is the most time consuming yet the most importat one. Here, we filter and extract only the information that is needed for problem solving. Quality of the model is highly dependant on the quality of the data that is given as an input. \n",
    "* Understand meaning of every feature and identify errors.\n",
    "* Look for any missing values and find a way to fill the missing values.\n",
    "* Remove duplicate or corrupted records.\n",
    "* Scaling and normalization of data.\n",
    "* Character encoding (string to numerical representation).\n",
    "* Handle inconsistent entry.\n",
    "* Use tools like pandas(python), dplyr(R), numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find number of missing values in every feature\n",
    "df_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the best dataset!!! No null values :-)\n",
    "\n",
    "#### But what if we have null values ???? Let's see what we can do in that case.\n",
    "\n",
    "* Find why that data is missing. Human error or missed during extraction\n",
    "* Drop missing values. \n",
    "* Some ways for filling missing values: \n",
    "  - Zero \n",
    "  - Mean ( works with normal distribution )\n",
    "  - Random values from same distribution ( works well with equal distribution ) \n",
    "  - Value after missing value (make sense if data set has some logical order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Encode categorical features(in string) as most of the tools works with numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns with string values\n",
    "categorical_column = ['Attrition', 'BusinessTravel', 'Department',\n",
    "                      'Gender', 'JobRole', 'MaritalStatus', 'OverTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep copy the original data\n",
    "data_encoded = df_data.copy(deep=True)\n",
    "#Use Scikit-learn label encoding to encode character data\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "for col in categorical_column:\n",
    "        data_encoded[col] = lab_enc.fit_transform(df_data[col])\n",
    "        le_name_mapping = dict(zip(lab_enc.classes_, lab_enc.transform(lab_enc.classes_)))\n",
    "        print('Feature', col)\n",
    "        print('mapping', le_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find patterns in data through data visualization. Reveal hidden secrets of the data through graphs, analysis and charts.\n",
    "   -  Univariate analysis \n",
    "      * Continous variables : Histograms, boxplots. This gives us understanding about the central tendency and spread\n",
    "      * Categorical variable : Bar chart showing frequency in each category \n",
    "   -  Bivariate analysis\n",
    "      * Continous & Continous : Scatter plots to know how continous variables interact with each other\n",
    "      * Categorical & categorical : Stacked column chart to show how the frequencies are spread between two  \n",
    "        categorical variables\n",
    "      * Categorical & Continous : Boxplots, Swamplots or even bar charts\n",
    "* Detect outliers\n",
    "* Feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Get data distribution between output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded['Attrition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, we can find that about 82% of people stick to the company while rest of them quit :-(\n",
    "\n",
    "\n",
    "**** Data is unbalanced ****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Finding correlation between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_correlation = data_encoded.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [15,10]\n",
    "sns.heatmap(data_correlation,xticklabels=data_correlation.columns,yticklabels=data_correlation.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of correlation results (sample analysis)\n",
    "\n",
    "- Monthly income is highly correlated with Job level.\n",
    "- Job level is highly correlated with total working hours.\n",
    "- Monthly income is highly correlated with total working hours.\n",
    "- Age is also positively correlated with the Total working hours.\n",
    "- Marital status and stock option level are negatively correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the analysis obtained above \n",
    "data_corr_filtered = df_data[['MonthlyIncome', 'TotalWorkingYears', 'Age', 'MaritalStatus', 'StockOptionLevel',\n",
    "                      'JobLevel']]\n",
    "correlation = data_corr_filtered.corr()\n",
    "plt.rcParams[\"figure.figsize\"] = [20,10]\n",
    "sns.heatmap(correlation,xticklabels=data_corr_filtered.columns,yticklabels=data_corr_filtered.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Understanding relationship between features and finding patterns in data through visualization\n",
    "\n",
    "Popular data visualization libraries in python are:\n",
    "     1. Matplotlib\n",
    "     2. Seaborn\n",
    "     3. ggplot\n",
    "     4. Bokeh\n",
    "     5. pygal\n",
    "     6. Plotly\n",
    "     7. geoplotlib\n",
    "     8. Gleam\n",
    "     9. missingno\n",
    "     10. Leather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Age Analysis\n",
    "Finding relationship between age and attrition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot to see distribution of age overall\n",
    "plt.rcParams[\"figure.figsize\"] = [7,7]\n",
    "plt.hist(data_encoded['Age'], bins=np.arange(0,80,10), alpha=0.8, rwidth=0.9, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding based on above plot\n",
    "This plot tells that there are more employees in the range of 30 to 40. Approximately 45% of employees fall in this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are going to bin age (multiples of 10) to see which age group are likely to leave the company.\n",
    "#Before that, let us take only employee who are likely to quit.\n",
    "positive_attrition_df = data_encoded.loc[data_encoded['Attrition'] == 1]\n",
    "negative_attrition_df = data_encoded.loc[data_encoded['Attrition'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(positive_attrition_df['Age'], bins=np.arange(0,80,10), alpha=0.8, rwidth=0.9, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings based on above plot\n",
    "- Employees whose age is in the range of 30 - 40 are more likely to quit.\n",
    "- Employees in the range of 20 to 30 are also equally imposing the threat to employers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Business Travel vs Attrition\n",
    "There are 3 categories in this:\n",
    "    1. No travel (0).\n",
    "    2. Travel Frequently (1).\n",
    "    3. Travel Rarely (2).\n",
    "Attrition: No = 0 and Yes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"BusinessTravel\", hue=\"Attrition\", data=data_encoded)\n",
    "for p in ax.patches:\n",
    "    ax.annotate('{}'.format(p.get_height()), (p.get_x(), p.get_height()+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "From the above plot it can be inferred that travel can not be a compelling factor for attrition. Employee who travel rarely are likely to quit more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Department Vs Attrition\n",
    "There are three categories in department:\n",
    "       1. Human Resources: 0\n",
    "       2. Research & Development: 1\n",
    "       3. Sales: 2\n",
    "Attrition: No = 0 and Yes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"Department\", hue=\"Attrition\", data=data_encoded)\n",
    "for p in ax.patches:\n",
    "    ax.annotate('{}'.format(p.get_height()), (p.get_x(), p.get_height()+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference:\n",
    "    1. 56% of employess from research and development department are likely to quit.\n",
    "    2. 38% of employees from sales department are likely to quit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Distance from home Vs Employee Attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(negative_attrition_df['DistanceFromHome'], bins=np.arange(0,80,10), alpha=0.8, rwidth=0.9, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(positive_attrition_df['DistanceFromHome'], bins=np.arange(0,80,10), alpha=0.8, rwidth=0.9, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "People who live closeby (0-10 miles) are likely to quit more based on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 Education vs Attrition\n",
    "There are five categories: \n",
    "     1. Below College - 1 \n",
    "     2. College - 2\n",
    "     3. Bachelor - 3\n",
    "     4. Master - 4\n",
    "     5. Doctor - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"Education\", hue=\"Attrition\", data=data_encoded)\n",
    "for p in ax.patches:\n",
    "    ax.annotate('{}'.format(p.get_height()), (p.get_x(), p.get_height()+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference:\n",
    "    1. 41% of employees having bachelor's degree are likely to quit.\n",
    "    2. 24% of employees having master's are the next in line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6 Gender vs Attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age = data_encoded.copy(deep=True)\n",
    "df_age.loc[df_age['Age'] <= 20, 'Age'] = 0\n",
    "df_age.loc[(df_age['Age'] > 20) & (df_age['Age'] <= 30), 'Age'] = 1\n",
    "df_age.loc[(df_age['Age'] > 30) & (df_age['Age'] <= 40), 'Age'] = 2\n",
    "df_age.loc[(df_age['Age'] > 40) & (df_age['Age'] <= 50), 'Age'] = 3\n",
    "df_age.loc[(df_age['Age'] > 50), 'Age'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age = pd.DataFrame({'count': df_age.groupby([\"Gender\", \"Attrition\"]).size()}).reset_index()\n",
    "df_age['Gender-attrition'] = df_age['Gender'].astype(str) + \"-\" + df_age['Attrition'].astype(str).map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,\n",
    "\n",
    "* Gender - 0 and Attrition - 0 ===> Female employees who will stay\n",
    "* Gender - 0 and Attrition - 1 ===> Female employees who will leave\n",
    "* Gender - 1 and Attrition - 0 ===> Male employees who will stay\n",
    "* Gender - 1 and Attrition - 1 ===> Male employees who will leave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook() \n",
    "\n",
    "# x and y axes\n",
    "Gender_Attrition = df_age['Gender-attrition'].tolist()\n",
    "count = df_age['count'].tolist()\n",
    "\n",
    "print(count)\n",
    "\n",
    "# Bokeh's mapping of column names and data lists\n",
    "source = ColumnDataSource(data=dict(Gender_Attrition=Gender_Attrition, count=count, color=Viridis5))\n",
    "\n",
    "plot_bar = figure(x_range=Gender_Attrition, plot_height=350, title=\"Counts\")\n",
    "\n",
    "# Render and show the vbar plot\n",
    "plot_bar.vbar(x='Gender_Attrition', top='count', width=0.9, color='color', source=source)\n",
    "show(plot_bar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "From the above plot, we can infer that male employees are likely to leave organization as they amount to 63% compared to female who have 36 % attrition rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.7 Job Role Vs Attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories in job role:\n",
    "* Healthcare Representative : 0 \n",
    "* Human Resources : 1\n",
    "* Laboratory Technician : 2\n",
    "* Manager : 3 \n",
    "* Manufacturing Director : 4\n",
    "* Research Director : 5\n",
    "* Research Scientist : 6\n",
    "* Sales Executive : 7 \n",
    "* Sales Representative : 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jrole = pd.DataFrame({'count': data_encoded.groupby([\"JobRole\", \"Attrition\"]).size()}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering attrition case\n",
    "df_jrole_1 = df_jrole.loc[df_jrole['Attrition'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygal\n",
    "chart = pygal.Bar(print_values=True)\n",
    "chart.x_labels = map(str, range(0,9))\n",
    "chart.add('Count', df_jrole_1['count'])\n",
    "#chart.render()\n",
    "display(SVG(chart.render(disable_xml_declaration=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "Top three roles facing attrition\n",
    "- 26% of employees who are likely to quit belong to Laboratory Technician group\n",
    "- 24% of employees belong to Sales Executive group\n",
    "- 19% of employees belong to Research Scientist group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.8 Marital Status vs Attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories:\n",
    "    1. 'Divorced': 0\n",
    "    2. 'Married' : 1\n",
    "    3. 'Single'  : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyzing employees who has positive attrition\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "positive_attrition_df['MaritalStatus'].value_counts().iplot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference:\n",
    "Nearly 50 % of the employees who are single are likely to quit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.9 Monthly Income vs Attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(negative_attrition_df['MonthlyIncome'], label='Negative attrition')\n",
    "sns.distplot(positive_attrition_df['MonthlyIncome'], label='positive attrition')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference:\n",
    "    Looks like people who are less likely to leave the company are the ones who are less paid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Extracting label from input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = data_encoded.drop(['Attrition'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = data_encoded[['Attrition']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the process of choosing the best features that can be used in the predictive modeling. \n",
    "\n",
    "1. Fight against the curse of dimensionality.\n",
    "2. Reduce the overall training time.\n",
    "3. Defense against overfitting.\n",
    "4. Increase model generalizability.\n",
    "\n",
    "Some of the feature selection methods are,\n",
    "1. Filter methods\n",
    "   - F Test\n",
    "   - Mutual information\n",
    "   - Variance threshold\n",
    "   - Chi Square\n",
    "   - Correlation coefficient \n",
    "   - ANNOVA\n",
    "   - LDA\n",
    "2. Wrapper methods\n",
    "   - Forward search\n",
    "   - Backward selection\n",
    "   - Recursive feature elimination\n",
    "3. Embedded methods\n",
    "   - LASSO Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_values = list(input_data.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual Information\n",
    "It measures the dependence of one variable to another. If,\n",
    "\n",
    "* Mutual information is 0, then variable X carries no information about the variable Y. X and Y are independent.\n",
    "* Mutual information is 1, then variable X can be determined from variable Y. X and Y are dependent.\n",
    "\n",
    "Features can be selected based on their mutual information value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives top 10 features having maximum mutual information value\n",
    "feature_scores = mutual_info_classif(input_data, target_data)\n",
    "for score, fname in sorted(zip(feature_scores, col_values), reverse=True)[:10]:\n",
    "    print(fname, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chi-square\n",
    "\n",
    "chi-square test is applied to test the independence of two events. This method is used to evaluate the likelihood of correlation or association between features using their frequency distribution. This works best with categorial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives top 10 features having maximum chi-square value\n",
    "feature_scores = chi2(input_data, target_data)[0]\n",
    "for score, fname in sorted(zip(feature_scores, col_values), reverse=True)[:10]:\n",
    "    print(fname, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column selection based on feature selection \n",
    "data_selected = df_data[['MonthlyIncome', 'TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole', \n",
    "                      'YearsWithCurrManager', 'Age', 'OverTime', 'DistanceFromHome', 'StockOptionLevel',\n",
    "                      'JobLevel', 'JobRole', 'WorkLifeBalance', 'Gender', 'Attrition']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_selected.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding labels\n",
    "data_selected.loc[data_selected.Attrition == 'No', 'Attrition'] = 0\n",
    "data_selected.loc[data_selected.Attrition == 'Yes', 'Attrition'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = data_selected.drop(['Attrition'], axis=1)\n",
    "target_data = data_selected[['Attrition']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Train, Validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = data_selected[0:1269]\n",
    "print('Shape of the input data is ', input_data.shape)\n",
    "input_data['Attrition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = data_selected[1269:1469]\n",
    "print('Shape of the validation data is ', validation_data.shape)\n",
    "validation_input_data = validation_data.drop(['Attrition'], axis=1)\n",
    "print('Shape of the validation input data is ', validation_input_data.shape)\n",
    "validation_target_data = validation_data[['Attrition']]\n",
    "print('Shape of the validation target data is ', validation_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using 1 sample as test data to check deployment\n",
    "test_data = data_selected[1469:]\n",
    "print(test_data)\n",
    "print('Shape of the test data is ', test_data.shape)\n",
    "test_input_data = test_data.drop(['Attrition'], axis=1)\n",
    "print('Shape of the test input data is ', test_input_data.shape)\n",
    "test_target_data = test_data[['Attrition']]\n",
    "print('Shape of the test target data is ', test_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/IBM/employee-attrition-aif360/raw/master/data/Pipeline_LabelEncoder-0.1.zip --output-document=Pipeline_LabelEncoder-0.1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pipeline_LabelEncoder-0.1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding training and validation data. \n",
    "#custom label encoder library\n",
    "from Pipeline_LabelEncoder.sklearn_label_encoder import PipelineLabelEncoder\n",
    "preprocessed_data = PipelineLabelEncoder(columns = ['OverTime', 'JobRole', 'Gender']).fit_transform(input_data)\n",
    "print('-------------------------')\n",
    "print('validation data encoding')\n",
    "validation_enc_data = PipelineLabelEncoder(columns = ['OverTime', 'JobRole', 'Gender']).transform(validation_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Bias Mitigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AI Fairness 360 toolkit is an open-source library to help detect and remove bias in machine learning models. The AI Fairness 360 Python package includes a comprehensive set of metrics for datasets and models to test for biases, explanations for these metrics, and algorithms to mitigate bias in datasets and models.\n",
    "\n",
    "- Github: https://github.com/IBM/AIF360\n",
    "- Interative experience: http://aif360.mybluemix.net/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert dataset into a format that can be used by bias mitigation algorithms\n",
    "\n",
    "We suspect that there is a bias present in the gender attribute. \n",
    "\n",
    "Intution: Female employees are given favourable outcome (no attrition) compared to male employees. \n",
    "\n",
    "\n",
    "Identify the following in the dataset:\n",
    "1. Favorable label\n",
    "2. Unfavorable label\n",
    "3. Privileged group\n",
    "4. Unprivileged group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender is the protected attribute. \n",
    "#label 0: Employee will stay \n",
    "#label 1: Employee will leave\n",
    "# Gender 0: Female and Gender 1: Male\n",
    "privileged_groups = [{'Gender': 0}]\n",
    "unprivileged_groups = [{'Gender': 1}]\n",
    "favorable_label = 0 \n",
    "unfavorable_label = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create binary label dataset that can be used by bias mitigation algorithms\n",
    "BM_dataset = BinaryLabelDataset(favorable_label=favorable_label,\n",
    "                                unfavorable_label=unfavorable_label,\n",
    "                                df=preprocessed_data,\n",
    "                                label_names=['Attrition'],\n",
    "                                protected_attribute_names=['Gender'],\n",
    "                                unprivileged_protected_attributes=unprivileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Training Data Details\"))\n",
    "print(\"shape of the training dataset\", BM_dataset.features.shape)\n",
    "print(\"Training data favorable label\", BM_dataset.favorable_label)\n",
    "print(\"Training data unfavorable label\", BM_dataset.unfavorable_label)\n",
    "print(\"Training data protected attribute\", BM_dataset.protected_attribute_names)\n",
    "print(\"Training data privileged protected attribute (1:Male and 0:Female)\", \n",
    "      BM_dataset.privileged_protected_attributes)\n",
    "print(\"Training data unprivileged protected attribute (1:Male and 0:Female)\",\n",
    "      BM_dataset.unprivileged_protected_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(BM_dataset, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % \n",
    "      metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative difference indicate the presence of bias. Refer above link to know more about the working of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "               privileged_groups=privileged_groups)\n",
    "RW.fit(BM_dataset)\n",
    "train_tf_dataset = RW.transform(BM_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(train_tf_dataset, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "      % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a highly unbalanced data. Class 0 covers 83.88% of the data whereas class 1 covers 16.12% of the data. Following are the few ways to handle unbalance data.\n",
    "\n",
    "1. Data Level Approach\n",
    "   - Random under-sampling\n",
    "   - Cluster based over sampling\n",
    "   - Synthetic Minority Over-sampling Technique\n",
    "   - Modified Synthetic Minority Over-sampling Technique\n",
    "2. Algorithm Ensemble\n",
    "   - Bagging\n",
    "   - Boosting\n",
    "     - Adaptive Boosting (Ada-boost)\n",
    "     - Gradient Tree Boosting\n",
    "     - XGBoost\n",
    "     \n",
    "     \n",
    "In our model, Ada-boost ensemble technique is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost Short Description\n",
    "\n",
    "Many weak and inaccurate classifiers are combined to produce a highly accurate prediction. The classifer is serially trained. Samples that are misclassified in previous round are given more focus. Initially weight is equal for all the samples. Weight of misclassified instances are increased each time and weight of correctly classified instances are decreased, this will let more misclassfied sampled to be selected for the next round. After each classifier is trained, the weight is assigned to the classifier as well based on accuracy. More accurate classifier is assigned higher weight so that it will have more impact in final outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_estimators is the maximum number of estimators at which the boosting is terminated. Default is 50 and this can be tuned as well.\n",
    "cls = AdaBoostClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification model can be evaluated using different metrics. Some of the important metrics are:\n",
    "    1. Confusion matrix\n",
    "    2. Accuracy\n",
    "    3. Precision\n",
    "    4. Recall\n",
    "    5. Specificity\n",
    "    6. F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding best estimator\n",
    "#tune other parameters for better accuracy\n",
    "estimator = [100, 150, 200, 250, 300, 400, 500, 700, 1000]\n",
    "for i in estimator:\n",
    "    cls = AdaBoostClassifier(n_estimators=i)\n",
    "    cls.fit(train_tf_dataset.features, train_tf_dataset.labels,sample_weight=train_tf_dataset.instance_weights)\n",
    "    print('--------------------------------------------------------------------------------------------')\n",
    "    print('------    Training Results for {} estimators   ---------'.format(i))\n",
    "    predicted_output = cls.predict(train_tf_dataset.features)\n",
    "    accuracy = metrics.accuracy_score(train_tf_dataset.labels, predicted_output)\n",
    "    print('Accuracy for {} estimators is {}'.format(i, accuracy))\n",
    "    print(classification_report(train_tf_dataset.labels, predicted_output))\n",
    "    print('------    Test Results for {} estimators   ---------'.format(i))\n",
    "    predicted_output = cls.predict(validation_enc_data)\n",
    "    accuracy = metrics.accuracy_score(validation_target_data, predicted_output)\n",
    "    print('Accuracy for {} estimators is {}'.format(i, accuracy))\n",
    "    print(classification_report(validation_target_data, predicted_output))\n",
    "    print('--------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the best estimator value from above and replace the 'num_of_estimators' with the value.\n",
    "num_of_estimators = 100\n",
    "cls = AdaBoostClassifier(n_estimators=num_of_estimators)\n",
    "cls.fit(train_tf_dataset.features, train_tf_dataset.labels,sample_weight=train_tf_dataset.instance_weights)\n",
    "#Creating model pipeline\n",
    "test_pp = PipelineLabelEncoder(columns = ['OverTime', 'JobRole', 'Gender'])\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', test_pp), \n",
    "                       ('classifier', cls)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better accuracy,\n",
    "1. Tune hyper-parameters.\n",
    "2. Try above mentioned methods for handling unbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.predict(test_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Factors contributing more to the employee attrition are MonthlyIncome, TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, \n",
    "   YearsWithCurrManager, Age, OverTime, DistanceFromHome, StockOptionLevel, JobLevel, JobRole, WorkLifeBalance, Gender.\n",
    "2. Top three roles facing attrition\n",
    "   - 26% of employees who are likely to quit belong to Laboratory Technician group.\n",
    "   - 24% of employees belong to Sales Executive group.\n",
    "   - 19% of employees belong to Research Scientist group.\n",
    "   (other inferences are mentioned below each graph)\n",
    "3. The model developed will be able to predict whether an employee will stay or not. This will help company to know the status of an     \n",
    "   employee in advance and take necessary actions to prevent loss that will incur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Deployment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate to the Watson Machine Learning service. Enter the credentials needed. Credentials can be retrieved from the 'Service Credentials' tab of the service instance instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/IBM/employee-attrition-aif360/raw/master/docs/source/images/deploy.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  \n",
    "Replace the information in the following cell with your Watson Machine Learning (WML) credentials.\n",
    "\n",
    "You can find these credentials in your WML instance dashboard under the Service credentials tab.\n",
    "\n",
    "```\n",
    "wml_credentials = {\n",
    "    \"username\": \"------------\",\n",
    "    \"password\": \"------------\",\n",
    "    \"instance_id\": \"------------\",\n",
    "    \"url\": \"------------\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "wml_credentials = {\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create WML API Client\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create metadata that can be used for creating and saving the custom library. Here, it is for pipeline label encoder.\n",
    "#Give the path of the custom package (zip). \n",
    "library_metadata = {\n",
    "        client.runtimes.LibraryMetaNames.NAME: \"PipelineLabelEncoder-Custom\",\n",
    "        client.runtimes.LibraryMetaNames.DESCRIPTION: \"label_encoder_sklearn\",\n",
    "        client.runtimes.LibraryMetaNames.FILEPATH: \"Pipeline_LabelEncoder-0.1.zip\",\n",
    "        client.runtimes.LibraryMetaNames.VERSION: \"1.0\",\n",
    "        client.runtimes.LibraryMetaNames.PLATFORM: {\"name\": \"python\", \"versions\": [\"3.5\"]}\n",
    "    }\n",
    "#Store library\n",
    "custom_library_details = client.runtimes.store_library(library_metadata)\n",
    "#Retrieve library uid from the details\n",
    "custom_library_uid = client.runtimes.get_library_uid(custom_library_details)\n",
    "print(\"Custom Library UID is: \" + custom_library_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define metadata required for creating runtime resource. Yse custom library uid obtained from above step to bind the custom library.\n",
    "#Runtime resource that is being defined here will be used for configuring online deployment runtime environment\n",
    "runtimes_meta = {\n",
    "    client.runtimes.ConfigurationMetaNames.NAME: \"Employee_Attrition\", \n",
    "    client.runtimes.ConfigurationMetaNames.DESCRIPTION: \"Data Science Life Cycle explained through employee attrition problem\", \n",
    "    client.runtimes.ConfigurationMetaNames.PLATFORM: { \"name\": \"python\", \"version\": \"3.5\" }, \n",
    "    client.runtimes.ConfigurationMetaNames.LIBRARIES_UIDS: [custom_library_uid]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create runtime resource\n",
    "runtime_resource_details = client.runtimes.store(runtimes_meta)\n",
    "runtime_resource_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the runtime resource retrieve url and uid \n",
    "runtime_url = client.runtimes.get_url(runtime_resource_details)\n",
    "print(\"Runtimes resource URL: \" + runtime_url)\n",
    "runtime_uid = client.runtimes.get_uid(runtime_resource_details)\n",
    "print(\"Runtimes resource UID: \" + runtime_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.repository is used for storing and managing the model, definitions, runtime requirements details in WML repository\n",
    "#This metadata associates model with runtime resources\n",
    "model_property = {client.repository.ModelMetaNames.NAME: \"Employee attrition Model\",\n",
    "               client.repository.ModelMetaNames.RUNTIME_UID: runtime_uid\n",
    "              }\n",
    "published_model = client.repository.store_model(model=model_pipeline, meta_props=model_property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get uid for the stored model\n",
    "published_model_uid = client.repository.get_model_uid(published_model)\n",
    "model_details = client.repository.get_details(published_model_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create deployment\n",
    "created_deployment = client.deployments.create(published_model_uid, name=\"Emp_attrition_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get scoring end point\n",
    "scoring_endpoint = client.deployments.get_scoring_url(created_deployment)\n",
    "print(scoring_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing deployment\n",
    "scoring_payload = {'fields': ['MonthlyIncome', 'TotalWorkingYears', 'YearsAtCompany',\n",
    "       'YearsInCurrentRole', 'YearsWithCurrManager', 'Age', 'OverTime',\n",
    "       'DistanceFromHome', 'StockOptionLevel', 'JobLevel', 'JobRole',\n",
    "       'WorkLifeBalance', 'Gender'], \n",
    "                   'values': [[4404, 6, 4, 3, 2, 34, 'No', '8', 0, 2, 'Laboratory Technician', 4, 'Female']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = client.deployments.score(scoring_endpoint, scoring_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('prediction',json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the results:**\n",
    "\n",
    "In the \"values\" list, the first value indicates the predicted result - \n",
    "    \n",
    "    0 represents negative employee attrition (employee will stay)\n",
    "    \n",
    "    1 represents positive employee attrition (employee will quit)\n",
    "\n",
    "\n",
    "Next in the list will be the probablities for each of the results -\n",
    "    \n",
    "    The first entry (class 0) represents probability of negative employee attrition\n",
    "    \n",
    "    The second entry (class 1) represents probability of positive employee attrition\n",
    "    \n",
    "    \n",
    "**Modify the values in \"scoring_payload\" and see if you can get the prediction to change.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Desktop/MAX_Demo_Files/Data_Science_Pipeline_Demo.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
